{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05109ba3",
   "metadata": {},
   "source": [
    "### Lo scopo di questo cosice Ã¨ quello di costruire una CNN per il riconoscimento dei caratteri hiragana (il dataset utilizzato viene costruito con il codice nel notebook Last Hiragana Dataset) tramite la libreria Tensorflow.\n",
    "\n",
    "### Tutti il convulutional layers della CNN hanno un dropout con coefficiente posto a 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecad17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'C:/Users/andma/OneDrive/Documenti/hiragana images/hiragana_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_path, color_mode='grayscale', labels = 'inferred', image_size=(84, 84), validation_split=0.15, seed=42, subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of elements in the dataset\n",
    "elements = tf.cast(dataset.cardinality(), tf.int64)\n",
    "num_elements = elements.numpy()\n",
    "\n",
    "# Split the dataset into a training set, a validation set, and a test set\n",
    "train_dataset = dataset.take(int(0.7 * num_elements))\n",
    "validation_dataset = dataset.skip(int(0.7 * num_elements)).take(int(0.15 * num_elements))\n",
    "test_dataset = dataset.skip(int(0.85 * num_elements))\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(\"Train dataset:\", train_dataset.element_spec)\n",
    "print(\"Validation dataset:\", validation_dataset.element_spec)\n",
    "print(\"Test dataset:\", test_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9984ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 84\n",
    "img_width = 84\n",
    "\n",
    "filters = 32\n",
    "num_classes = 50\n",
    "dropout_coeff = 0.2\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "##### Rescaling layer\n",
    "model.add(Rescaling(1./255, input_shape=(img_height, img_width, 1)))\n",
    "\n",
    "##### First convolution layer\n",
    "model.add(Conv2D(filters, (3,3)))  #, input_shape = x_trainr.shape[1:]\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(dropout_coeff, input_shape=(39, 39)))\n",
    "\n",
    "##### Second convolution layer\n",
    "model.add(Conv2D(filters, (3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(dropout_coeff, input_shape=(17, 17)))\n",
    "\n",
    "##### Third convolution layer\n",
    "model.add(Conv2D(filters, (3,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(dropout_coeff, input_shape=(8, 8)))\n",
    "\n",
    "##### Fully Connected Layer #1\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "##### Fully Connected Layer #2\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "##### Last Fully Connected Layer, 50 Outputs\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Compiling the Model\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28251838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traing with the second splitting of the dataset\n",
    "epochs_data2 = 15\n",
    "\n",
    "history_2 = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9044a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loss = ', history_2.history['loss'][-1])\n",
    "print('accuracy = ', history_2.history['accuracy'][-1])\n",
    "print('val_loss = ', history_2.history['val_loss'][-1])\n",
    "print('val_accuracy = ', history_2.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ce84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = list(range(0, epochs_data2))\n",
    "plt.plot(epoch_num, history_2.history['loss'], label = 'loss')\n",
    "plt.plot(epoch_num, history_2.history['accuracy'], label = 'accuracy')\n",
    "plt.plot(epoch_num, history_2.history['val_loss'], label = 'val_loss')\n",
    "plt.plot(epoch_num, history_2.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loss = ', history_2.history['loss'][-1])\n",
    "print('accuracy = ', history_2.history['accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = list(range(0, epochs_data2))\n",
    "plt.plot(epoch_num, history_2.history['loss'], label = 'loss')\n",
    "plt.plot(epoch_num, history_2.history['accuracy'], label = 'accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
